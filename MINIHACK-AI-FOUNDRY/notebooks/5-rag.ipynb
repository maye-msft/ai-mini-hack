{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf62fbc",
   "metadata": {},
   "source": [
    "# Basic Retrieval-Augmented Generation (RAG) with AIProjectClient\n",
    "\n",
    "In this notebook, we'll demonstrate a **basic RAG** flow using:\n",
    "- **`azure-ai-projects`** (AIProjectClient)\n",
    "- **`azure-ai-inference`** (Embeddings, ChatCompletions)\n",
    "- **`azure-ai-search`** (for vector or hybrid search)\n",
    "- **RAG Mini Wikipedia dataset** - A curated dataset perfect for learning RAG concepts\n",
    "\n",
    "We'll use the **RAG Mini Wikipedia** dataset which contains 3,200+ Wikipedia passages and Q&A pairs, providing a rich knowledge base for our RAG system.\n",
    "\n",
    "## What is RAG?\n",
    "Retrieval-Augmented Generation (RAG) is a technique where the LLM (Large Language Model) uses relevant retrieved text chunks from your data to craft a final answer. This helps ground the model's response in real data, reducing hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfbd81",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We'll import libraries, load environment variables, and create an `AIProjectClient`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7395b2e",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# azure-ai-projects\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# For vector search or hybrid search\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "conn_string = os.environ.get(\"AIPROJECT_ENDPOINT\")\n",
    "chat_model = os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "embedding_model = os.environ.get(\"EMBEDDING_MODEL_DEPLOYMENT_NAME\", \"text-embedding-3-small\")\n",
    "search_index_name = os.environ.get(\"SEARCH_INDEX_NAME\", \"rag-mini-wikipedia-index\")\n",
    "\n",
    "try:\n",
    "    project_client = AIProjectClient(\n",
    "        credential=DefaultAzureCredential(),\n",
    "        endpoint=os.environ[\"AIPROJECT_ENDPOINT\"],\n",
    "    )\n",
    "    print(\"‚úÖ AIProjectClient created successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error creating AIProjectClient:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b84bb8",
   "metadata": {},
   "source": [
    "## Load RAG Mini Wikipedia Dataset\n",
    "We'll load the RAG Mini Wikipedia dataset which contains high-quality Wikipedia passages perfect for RAG learning. This dataset provides:\n",
    "- **3,200+ text passages** covering diverse topics\n",
    "- **900+ Q&A pairs** for testing and evaluation\n",
    "- **Clean, structured content** ready for embedding and search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the RAG Mini Wikipedia dataset\n",
    "print(\"üì• Loading RAG Mini Wikipedia dataset...\")\n",
    "\n",
    "# Initialize variables\n",
    "wikipedia_docs = []\n",
    "qa_pairs = []\n",
    "\n",
    "try:\n",
    "    # Load text corpus\n",
    "    print(\"üîÑ Loading text corpus...\")\n",
    "    text_corpus_dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\")\n",
    "    text_corpus = text_corpus_dataset[\"passages\"]\n",
    "    \n",
    "    # Load Q&A pairs \n",
    "    print(\"üîÑ Loading Q&A pairs...\")\n",
    "    qa_dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\")\n",
    "    qa_pairs_dataset = qa_dataset[\"test\"]\n",
    "\n",
    "    \n",
    "    # Convert to our format (taking first 100 documents for this demo)\n",
    "    sample_size = min(100, len(text_corpus))\n",
    "    for i in range(sample_size):\n",
    "        doc = text_corpus[i]\n",
    "        \n",
    "        # Handle different possible data structures\n",
    "        if isinstance(doc, dict):\n",
    "            content = doc.get(\"passage\", doc.get(\"text\", str(doc)))\n",
    "        elif isinstance(doc, str):\n",
    "            content = doc\n",
    "        else:\n",
    "            content = str(doc)\n",
    "            \n",
    "        wikipedia_docs.append({\n",
    "            \"id\": f\"doc_{i}\",\n",
    "            \"content\": content,\n",
    "            \"source\": \"Wikipedia\"\n",
    "        })\n",
    "    \n",
    "    # Process Q&A pairs (taking first 50 pairs for this demo)\n",
    "    qa_sample_size = min(50, len(qa_pairs_dataset))\n",
    "    for i in range(qa_sample_size):\n",
    "        qa_item = qa_pairs_dataset[i]\n",
    "        \n",
    "        # Handle different possible data structures\n",
    "        if isinstance(qa_item, dict):\n",
    "            question = qa_item.get(\"question\", \"\")\n",
    "            answer = qa_item.get(\"answer\", \"\")\n",
    "        else:\n",
    "            # Skip if not in expected format\n",
    "            continue\n",
    "            \n",
    "        qa_pairs.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    \n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìö Knowledge base: {len(wikipedia_docs)} documents (from {len(text_corpus)} total)\")\n",
    "    print(f\"‚ùì Q&A pairs: {len(qa_pairs)} pairs available for testing\")\n",
    "    \n",
    "    # Show a sample document\n",
    "    if wikipedia_docs:\n",
    "        print(f\"\\nüìÑ Sample document:\")\n",
    "        print(f\"Content: {wikipedia_docs[0]['content'][:200]}...\")\n",
    "    \n",
    "    # Show a sample Q&A pair\n",
    "    if qa_pairs:\n",
    "        print(f\"\\n‚ùì Sample Q&A:\")\n",
    "        print(f\"Q: {qa_pairs[0]['question']}\")\n",
    "        print(f\"A: {qa_pairs[0]['answer']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    print(f\"üîç Error type: {type(e)}\")\n",
    "    \n",
    "\n",
    "print(f\"\\nüéØ Ready to proceed with {len(wikipedia_docs)} documents!\")\n",
    "print(f\"üìä Available Q&A pairs: {len(qa_pairs)}\")\n",
    "\n",
    "# Show sample data regardless of source\n",
    "if wikipedia_docs:\n",
    "    print(f\"\\nüìÑ First document preview:\")\n",
    "    print(f\"ID: {wikipedia_docs[0]['id']}\")\n",
    "    print(f\"Content: {wikipedia_docs[0]['content'][:150]}...\")\n",
    "    print(f\"Source: {wikipedia_docs[0]['source']}\")\n",
    "\n",
    "if qa_pairs:\n",
    "    print(f\"\\n‚ùì First Q&A pair:\")\n",
    "    print(f\"Q: {qa_pairs[0]['question']}\")\n",
    "    print(f\"A: {qa_pairs[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b8d39",
   "metadata": {},
   "source": [
    "## Create Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd84b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "\n",
    "def create_wikipedia_index(\n",
    "        endpoint: str, api_key: str, index_name: str, \n",
    "        dimension: int = 1536 # if using text-embedding-3-small\n",
    "        ):\n",
    "    \"\"\"Create or update a search index for Wikipedia docs with vector search capability.\"\"\"\n",
    "    \n",
    "    index_client = SearchIndexClient(endpoint=endpoint, credential=AzureKeyCredential(api_key))\n",
    "    \n",
    "    # Try to delete existing index\n",
    "    try:\n",
    "        index_client.delete_index(index_name)\n",
    "        print(f\"Deleted existing index: {index_name}\")\n",
    "    except Exception:\n",
    "        pass  # Index doesn't exist yet\n",
    "        \n",
    "    # Define vector search configuration\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"myHnsw\",\n",
    "                kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    ef_construction=400,\n",
    "                    ef_search=500,\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Define fields\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"source\", type=SearchFieldDataType.String),\n",
    "        SearchField(\n",
    "            name=\"embedding\", \n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            vector_search_dimensions=dimension,\n",
    "            vector_search_profile_name=\"myHnswProfile\" \n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # Create index definition\n",
    "    index_def = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search\n",
    "    )\n",
    "    \n",
    "    # Create the index\n",
    "    index_client.create_index(index_def)\n",
    "    print(f\"‚úÖ Created or reset index: {index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b4d51a",
   "metadata": {},
   "source": [
    "## Upload Wikipedia Documents\n",
    "\n",
    "Now we'll put our Wikipedia knowledge into action by:\n",
    "1. **Creating a search connection** to Azure AI Search\n",
    "2. **Building our index** with vector search capability\n",
    "3. **Generating embeddings** for each Wikipedia document\n",
    "4. **Uploading** the documents with their embeddings\n",
    "\n",
    "This creates our knowledge base that we'll search through later. Think of it as building our 'Wikipedia library' that our AI assistant can reference! üìöüîç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f2f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import ConnectionType\n",
    "\n",
    "# Step 1: Get search connection\n",
    "search_conn = project_client.connections.get_default(\n",
    "    connection_type=ConnectionType.AZURE_AI_SEARCH, \n",
    "    include_credentials=True\n",
    ")\n",
    "if not search_conn:\n",
    "    raise RuntimeError(\"‚ùå No default Azure AI Search connection found!\")\n",
    "\n",
    "print(\"‚úÖ Got search connection\")\n",
    "\n",
    "# Step 2: Get embeddings client and check embedding length\n",
    "embeddings_client = project_client.inference.get_embeddings_client()\n",
    "\n",
    "print(\"‚úÖ Created embeddings client\")\n",
    "\n",
    "sample_doc = wikipedia_docs[0]\n",
    "emb_response = embeddings_client.embed(\n",
    "        model=embedding_model,\n",
    "        input=[sample_doc[\"content\"]]\n",
    "    )\n",
    "embedding_length = len(emb_response.data[0].embedding)\n",
    "print(f\"‚úÖ Got embedding length: {embedding_length}\")\n",
    "\n",
    "# Step 3: Create the index\n",
    "create_wikipedia_index(\n",
    "    endpoint=search_conn.target,\n",
    "    api_key=search_conn.credentials['key'],\n",
    "    index_name=search_index_name,\n",
    "    dimension=embedding_length   \n",
    ")\n",
    "\n",
    "# Step 4: Create search client for uploading documents\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_conn.target,\n",
    "    index_name=search_index_name,\n",
    "    credential=AzureKeyCredential(search_conn.credentials['key'])\n",
    ")\n",
    "print(\"‚úÖ Created search client\")\n",
    "\n",
    "# Step 5: Embed and upload documents\n",
    "search_docs = []\n",
    "for doc in wikipedia_docs:\n",
    "    # Get embedding for document content\n",
    "    emb_response = embeddings_client.embed(\n",
    "        model=embedding_model,\n",
    "        input=[doc[\"content\"]]\n",
    "    )\n",
    "    emb_vec = emb_response.data[0].embedding\n",
    "    \n",
    "    # Create document with embedding\n",
    "    search_docs.append({\n",
    "        \"id\": doc[\"id\"],\n",
    "        \"content\": doc[\"content\"],\n",
    "        \"source\": doc[\"source\"],\n",
    "        \"embedding\": emb_vec,\n",
    "    })\n",
    "\n",
    "# Upload documents to index\n",
    "result = search_client.upload_documents(documents=search_docs)\n",
    "print(f\"‚úÖ Uploaded {len(search_docs)} documents to search index '{search_index_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e5468",
   "metadata": {},
   "source": [
    "## Basic RAG Flow\n",
    "### Retrieve\n",
    "When a user queries, we:\n",
    "1. Embed user question.\n",
    "2. Search vector index with that embedding to get top docs.\n",
    "\n",
    "### Generate answer\n",
    "We then pass the retrieved docs to the chat model.\n",
    "\n",
    "> In a real scenario, you'd have a more advanced approach to chunking & summarizing. We'll keep it simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "def rag_chat(query: str, top_k: int = 3) -> str:\n",
    "    # 1) Embed user query\n",
    "    user_vec = embeddings_client.embed(\n",
    "        model=embedding_model,\n",
    "        input=[query]).data[0].embedding\n",
    "\n",
    "    # 2) Vector search using VectorizedQuery\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=user_vec,\n",
    "        k_nearest_neighbors=top_k,\n",
    "        fields=\"embedding\"\n",
    "    )\n",
    "\n",
    "    results = search_client.search(\n",
    "        search_text=\"\",  # Optional text query\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"content\", \"source\"]  # Only retrieve fields we need\n",
    "    )\n",
    "\n",
    "    # gather the top docs\n",
    "    top_docs_content = []\n",
    "    for r in results:\n",
    "        c = r[\"content\"]\n",
    "        s = r[\"source\"]\n",
    "        top_docs_content.append(f\"Source: {s} => {c}\")\n",
    "\n",
    "    # 3) Chat with retrieved docs\n",
    "    system_text = (\n",
    "        \"You are a helpful assistant that answers questions using Wikipedia knowledge.\\n\"\n",
    "        \"Answer user questions using ONLY the text from these docs.\\n\"\n",
    "        \"Docs:\\n\"\n",
    "        + \"\\n\".join(top_docs_content)\n",
    "        + \"\\nIf unsure, say 'I'm not sure based on the provided information'.\\n\"\n",
    "    )\n",
    "\n",
    "    with project_client.inference.get_chat_completions_client() as chat_client:\n",
    "        response = chat_client.complete(\n",
    "            model=chat_model,\n",
    "            messages=[\n",
    "                SystemMessage(content=system_text),\n",
    "                UserMessage(content=query)\n",
    "            ]\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfecfb3c",
   "metadata": {},
   "source": [
    "## Try a Query üéâ\n",
    "Let's ask a question that might be answered by our Wikipedia knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3937fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is Uruguay known for?\"\n",
    "answer = rag_chat(user_query)\n",
    "print(\"üó£Ô∏è User Query:\", user_query)\n",
    "print(\"ü§ñ RAG Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e562e6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We've demonstrated a **basic RAG** pipeline with:\n",
    "- **Loading** the RAG Mini Wikipedia dataset with 3,200+ passages\n",
    "- **Embedding** Wikipedia docs & storing them in **Azure AI Search**\n",
    "- **Retrieving** top relevant docs for user questions\n",
    "- **Chat** with the retrieved Wikipedia knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pq6pisfgda",
   "metadata": {},
   "source": [
    "## RAG-Enhanced AI Agent ü§ñüìö\n",
    "\n",
    "Now let's create an **AI Agent that uses RAG** to answer questions about our Wikipedia knowledge base. This agent will:\n",
    "\n",
    "1. **Search** our Wikipedia vector index for relevant documents\n",
    "2. **Use the retrieved knowledge** to provide informed answers\n",
    "3. **Maintain conversation context** across multiple interactions\n",
    "\n",
    "This combines the power of AI agents with retrieval-augmented generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k6h27g2fxbp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official Azure AI Search Agent Demo\n",
    "# Based on: https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/azure-ai-search-samples\n",
    "\n",
    "from azure.ai.agents.models import AzureAISearchTool, ToolSet, ConnectionType\n",
    "\n",
    "# Get the search connection and index name\n",
    "search_connection = project_client.connections.get_default(\n",
    "    connection_type=ConnectionType.AZURE_AI_SEARCH,\n",
    "    include_credentials=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Using search connection: {search_connection.name}\")\n",
    "print(f\"üîç Index: {search_index_name}\")\n",
    "\n",
    "# Create the Azure AI Search tool with required parameters\n",
    "search_tool = AzureAISearchTool(\n",
    "    index_connection_id=search_connection.name,\n",
    "    index_name=search_index_name\n",
    ")\n",
    "\n",
    "# Create toolset and add the search tool\n",
    "toolset = ToolSet()\n",
    "toolset.add(search_tool)\n",
    "\n",
    "# Create agent with search capabilities\n",
    "agent = project_client.agents.create_agent(\n",
    "    model=chat_model,\n",
    "    name=\"search-agent\",\n",
    "    instructions=f\"\"\"You are a helpful assistant that can search for information using Azure AI Search.\n",
    "\n",
    "You have access to a Wikipedia knowledge base through the '{search_index_name}' index.\n",
    "    \n",
    "When users ask questions:\n",
    "1. Use the Azure AI Search tool to find relevant information\n",
    "2. Provide clear, accurate answers based on the search results\n",
    "3. If no relevant information is found, let the user know\n",
    "4. Be conversational and helpful\"\"\",\n",
    "    toolset=toolset,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Created agent: {agent.id}\")\n",
    "\n",
    "# Create a thread for the conversation\n",
    "thread = project_client.agents.threads.create()\n",
    "print(f\"‚úÖ Created thread: {thread.id}\")\n",
    "\n",
    "print(\"\\\\nüéØ Agent ready to answer questions using Azure AI Search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29wcupxwpjm",
   "metadata": {},
   "source": [
    "## Register Function Tool and Test the RAG Agent\n",
    "\n",
    "Now we need to register our search function so the agent can call it, and then test our RAG-enhanced agent with some questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bkldkd61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to chat with the agent\n",
    "def chat_with_agent(question: str):\n",
    "    \"\"\"Send a question to the agent and get a response.\"\"\"\n",
    "    \n",
    "    # Create message\n",
    "    message = project_client.agents.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=question\n",
    "    )\n",
    "    \n",
    "    # Run the agent\n",
    "    run = project_client.agents.runs.create_and_process(\n",
    "        thread_id=thread.id,\n",
    "        agent_id=agent.id\n",
    "    )\n",
    "    \n",
    "    print(f\"üë§ Question: {question}\")\n",
    "    print(f\"üîÑ Status: {run.status}\")\n",
    "    \n",
    "    if run.status == \"completed\":\n",
    "        # Get messages and display the response\n",
    "        messages = project_client.agents.messages.list(thread_id=thread.id)\n",
    "        for message in messages:\n",
    "            if message.role == \"assistant\":\n",
    "                for content in message.content:\n",
    "                    if content.type == \"text\":\n",
    "                        print(f\"ü§ñ Answer: {content.text.value}\")\n",
    "                        return\n",
    "    else:\n",
    "        print(f\"‚ùå Run failed: {run.last_error}\")\n",
    "\n",
    "# Demo: Test the agent with questions about our Wikipedia knowledge base\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üß™ DEMO: Agent with Azure AI Search\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test questions\n",
    "questions = [\n",
    "    \"Tell me about Uruguay\",\n",
    "    \"What information do you have about Abraham Lincoln?\",\n",
    "    \"What is the capital of Uruguay?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\\\nüìã Test {i}:\")\n",
    "    chat_with_agent(question)\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
