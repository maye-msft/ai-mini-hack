{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from typing import Annotated, Awaitable, Callable, TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncAzureOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    AzureTextEmbedding,\n",
    "    OpenAIEmbeddingPromptExecutionSettings,\n",
    ")\n",
    "from semantic_kernel.connectors.ai.prompt_execution_settings import PromptExecutionSettings\n",
    "from semantic_kernel.connectors.memory.azure_ai_search import (\n",
    "    AzureAISearchCollection,\n",
    "    AzureAISearchStore,\n",
    ")\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.core_plugins import HttpPlugin, MathPlugin\n",
    "from semantic_kernel.data import (\n",
    "    VectorStoreRecordDataField,\n",
    "    VectorStoreRecordKeyField,\n",
    "    VectorStoreRecordVectorField,\n",
    "    vectorstoremodel,\n",
    ")\n",
    "from semantic_kernel.filters import FilterTypes, FunctionInvocationContext\n",
    "from semantic_kernel.functions import (\n",
    "    kernel_function,\n",
    "    KernelArguments,\n",
    "    KernelParameterMetadata,\n",
    ")\n",
    "from semantic_kernel.prompt_template import KernelPromptTemplate, PromptTemplateConfig\n",
    "\n",
    "# Setup\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c7b60c",
   "metadata": {},
   "source": [
    "# Step 1: Create the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd2d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475a796",
   "metadata": {},
   "source": [
    "### üß™ Try It Out!\n",
    "\n",
    "Below is a chat function using the kernel. With just the **kernel**, try to chat and observe the behavior.\n",
    "\n",
    "> üìù *Notice that you cannot do much ‚Äî it's limited without additional orchestration or tools.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3606a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chat function\n",
    "async def chat():\n",
    "\n",
    "    # Prepare the prompt configuration\n",
    "    template = \"\"\"\n",
    "    You are a helpful assistant. Solve the problems for the user using tools when possible. \n",
    "\n",
    "    Previous information from chat:\n",
    "    {{$chat_history}}\n",
    "\n",
    "    User: {{$user_input}}\n",
    "    Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the chat history\n",
    "    chat_history = ChatHistory()\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter your message:\")\n",
    "        print(f\"\\nYou: {user_input}\")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        result = await kernel.invoke_prompt(\n",
    "            prompt=template,\n",
    "            arguments=KernelArguments(\n",
    "                PromptExecutionSettings(function_choice_behavior=FunctionChoiceBehavior.Auto()), \n",
    "                user_input=user_input,\n",
    "                chat_history = chat_history\n",
    "            )\n",
    "        )\n",
    "\n",
    "        assistant_reply = str(result)\n",
    "\n",
    "        chat_history.add_user_message(user_input)\n",
    "        chat_history.add_assistant_message(assistant_reply)\n",
    "        \n",
    "        print(f\"Bot: {assistant_reply}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f788e93",
   "metadata": {},
   "source": [
    "# Step 2: Add a chat completion service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_openai_client = AsyncAzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"MODEL_DEPLOYMENT_API_VERSION\"),\n",
    ")\n",
    "\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=\"azure_openai_chat\",\n",
    "        async_client=async_openai_client,\n",
    "        deployment_name=os.getenv(\"MODEL_DEPLOYMENT_NAME\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# üöÄ Test It!\n",
    "print(\"--- üìã Results ---\")\n",
    "prompt = \"You are a helpful assistant. What's the capital of Japan?\"\n",
    "result = await kernel.invoke_prompt(\n",
    "    prompt=prompt,\n",
    "    service_id=\"azure_openai_chat\"\n",
    ")\n",
    "print(str(result)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c342c55a",
   "metadata": {},
   "source": [
    "### üß™ Try It Out!\n",
    "\n",
    "Now that you've added a **chat completion service**, you can chat with the kernel!\n",
    "\n",
    "> üß† Keep in mind: it can only respond based on what it was trained on ‚Äî it doesn't know real-time facts.\n",
    "\n",
    "üîç **Try asking:**  \n",
    "> *\"What‚Äôs the date today?\"*\n",
    "\n",
    "You‚Äôll see that the kernel can‚Äôt answer accurately ‚Äî yet. Let‚Äôs fix that next!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c2618",
   "metadata": {},
   "source": [
    "# Step 3: Add some plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c8b96",
   "metadata": {},
   "source": [
    "- ## Add a simple function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28645618",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kernel_function(description=\"Get the current date\")\n",
    "def get_current_date():\n",
    "    return datetime.now()\n",
    "    \n",
    "get_current_date_fn = kernel.add_function(\n",
    "    plugin_name=\"Utils\",\n",
    "    function=get_current_date\n",
    ")\n",
    "\n",
    "# üöÄ Test It!\n",
    "print(\"--- üìã Results ---\")\n",
    "result = await kernel.invoke(function=get_current_date_fn)\n",
    "print(str(result)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6b530",
   "metadata": {},
   "source": [
    "- ## Add a plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa0cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brightness(Enum):\n",
    "    LOW = \"low\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HIGH = \"high\"\n",
    "\n",
    "class LightModel(TypedDict):\n",
    "    id: int\n",
    "    name: str\n",
    "    is_on: bool | None\n",
    "    brightness: Brightness | None\n",
    "    color: Annotated[str | None, \"The color of the light with a hex code (ensure you include the # symbol)\"]\n",
    "\n",
    "class LightsPlugin:\n",
    "    def __init__(self, lights: list[LightModel]):\n",
    "        self._lights = lights\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_lights(self) -> list[LightModel]:\n",
    "        \"\"\"Gets a list of lights and their current state.\"\"\"\n",
    "        return self._lights\n",
    "\n",
    "    @kernel_function\n",
    "    async def change_state(\n",
    "        self,\n",
    "        change_state: LightModel\n",
    "    ) -> LightModel | None:\n",
    "        \"\"\"Changes the state of the light.\"\"\"\n",
    "        for light in self._lights:\n",
    "            if light[\"id\"] == change_state[\"id\"]:\n",
    "                light[\"is_on\"] = change_state.get(\"is_on\", light[\"is_on\"])\n",
    "                light[\"brightness\"] = change_state.get(\"brightness\", light[\"brightness\"])\n",
    "                light[\"hex\"] = change_state.get(\"hex\", light[\"hex\"])\n",
    "                return light\n",
    "        return None\n",
    "\n",
    "# Create dependencies for the plugin\n",
    "lights = [\n",
    "    {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": False, \"brightness\": 100, \"hex\": \"FF0000\"},\n",
    "    {\"id\": 2, \"name\": \"Porch light\", \"is_on\": False, \"brightness\": 50, \"hex\": \"00FF00\"},\n",
    "    {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": True, \"brightness\": 75, \"hex\": \"0000FF\"},\n",
    "]\n",
    "\n",
    "# Create the plugin\n",
    "lights_plugin = LightsPlugin(lights)\n",
    "\n",
    "# Add the plugin to the kernel\n",
    "kernel.add_plugin(lights_plugin)\n",
    "\n",
    "# üöÄ Test It!\n",
    "print(\"--- üìã Results ---\")\n",
    "result = await kernel.invoke(\n",
    "    plugin_name=\"LightsPlugin\",\n",
    "    function_name=\"get_lights\"\n",
    ")\n",
    "print(str(result)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f06180",
   "metadata": {},
   "source": [
    "- ## Add an OpenApi plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin = kernel.add_plugin_from_openapi(\n",
    "    plugin_name=\"Currency\",\n",
    "    openapi_document_path=\"../data/exchangerate_api.yaml\",\n",
    "    description=\"Currency conversion tool\",\n",
    ")\n",
    "\n",
    "# üöÄ Test It!\n",
    "print(\"--- üìã Results ---\")\n",
    "args = KernelArguments()\n",
    "args[\"from\"] = \"USD\"\n",
    "args[\"to\"] = \"JPY\"\n",
    "args[\"amount\"] = 10.0\n",
    "result = await kernel.invoke(\n",
    "    plugin_name=\"Currency\",\n",
    "    function_name=\"convertCurrency\",\n",
    "    arguments=args\n",
    ")\n",
    "print(str(result)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2838288e",
   "metadata": {},
   "source": [
    "- ## Add a function from prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bde78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a semantic kernel core plugin\n",
    "kernel.add_plugin(HttpPlugin(), \"http\")\n",
    "\n",
    "# Defining a function from prompt template\n",
    "function_definition = \"\"\"\n",
    "The weather is: {{http.getAsync \"https://wttr.in/?format=4\"}}\n",
    "\n",
    "Answer in the format:\n",
    "Location: Tokyo, Japan\n",
    "Weather: Sunny (Add an emoji)\n",
    "Temperature: +30¬∞C\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = KernelPromptTemplate(prompt_template_config=PromptTemplateConfig(template=function_definition))\n",
    "kind_of_day = kernel.add_function(\n",
    "    plugin_name=\"TimePlugin\",\n",
    "    function_name=\"kind_of_day\",\n",
    "    description=\"Describe the weather.\",\n",
    "    prompt_template=prompt_template,\n",
    ")\n",
    "\n",
    "# üöÄ Test It!\n",
    "print(\"--- üìã Results ---\")\n",
    "result = await kernel.invoke(function=kind_of_day)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db8c0a",
   "metadata": {},
   "source": [
    "### üß™ Try it out!\n",
    "\n",
    "Now that you've added several plugins, try chatting with the kernel and see what it can do!\n",
    "\n",
    "You now have access to the following capabilities:\n",
    "\n",
    "- üìÖ **Get the current date**\n",
    "- üí° **Control smart lights**\n",
    "- üí± **Convert currency values**\n",
    "- üå§Ô∏è **Get the current weather for your location**\n",
    "\n",
    "üí¨ Try asking questions like:\n",
    "\n",
    "- *\"What is the date today?\"*\n",
    "- *\"Turn on the table lamp and set its brightness to high.\"*\n",
    "- *\"How much is 10 USD in JPY?\"*\n",
    "- *\"What's the weather like right now?\"*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d84417b",
   "metadata": {},
   "source": [
    "# Step 4: Add some filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb81c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def logger_filter(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: Callable[[FunctionInvocationContext], Awaitable[None]]\n",
    ") -> None:\n",
    "    plugin = context.function.plugin_name\n",
    "    function = context.function.name\n",
    "    args = context.arguments\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    await next(context)\n",
    "\n",
    "    if plugin:\n",
    "        print(\"\\n\\033[95m\" + \"‚ïê\" * 60)\n",
    "        print(f\"\\033[90müïí [{timestamp}]\\033[0m\")\n",
    "        print(f\"üîß \\033[94mFunction Invoked:\\033[0m \\033[93m{plugin}.{function} with args{args}\\033[0m\")\n",
    "        print(f\"üì§ \\033[94mResult:\\033[0m {context.result}\")\n",
    "        print(\"\\033[95m\" + \"‚ïê\" * 60 + \"\\033[0m\\n\")\n",
    "\n",
    "kernel.add_filter(FilterTypes.FUNCTION_INVOCATION, logger_filter)\n",
    "\n",
    "# üöÄ Test It!\n",
    "print(\"--- üìã Results ---\")\n",
    "\n",
    "kernel.add_plugin(MathPlugin(), plugin_name=\"math\")\n",
    "args = KernelArguments()\n",
    "args[\"input\"] = 1\n",
    "args[\"amount\"] = 1\n",
    "result = await kernel.invoke(plugin_name=\"math\", function_name=\"Add\", arguments=args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0872b32",
   "metadata": {},
   "source": [
    "\n",
    "### üß™ Try it out! \n",
    "Now that you‚Äôve added a logging filter to the kernel, try chatting with it again!\n",
    "Observe how the log shows which functions are being invoked and their results. This helps you **debug**, **understand tool usage**, and **trace behavior** during execution.\n",
    "\n",
    "üéØ Bonus Challenge\n",
    "\n",
    "Add a **prompt rendering filter** to your kernel!\n",
    "\n",
    "üõ†Ô∏è Try to log the following:\n",
    "\n",
    "- üìù The fully rendered prompt text  \n",
    "- ü§ñ Which function is generating the prompt  \n",
    "- üì¶ Any input arguments passed\n",
    "\n",
    "This will give you complete visibility into the kernel‚Äôs orchestration behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e91ccf",
   "metadata": {},
   "source": [
    "# Step 5: Add memory connector to perform RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8005ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a text embedding service\n",
    "kernel.add_service(\n",
    "    AzureTextEmbedding(\n",
    "        service_id=\"azure_openai_embeddings\",\n",
    "        async_client=async_openai_client,\n",
    "        deployment_name=os.getenv(\"EMBEDDING_MODEL_DEPLOYMENT_NAME\"),\n",
    "    ),\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "\n",
    "# Define the data model\n",
    "@vectorstoremodel\n",
    "class HotelBrochures(BaseModel):\n",
    "    id: Annotated[str, VectorStoreRecordKeyField()]\n",
    "    city: Annotated[str | None, VectorStoreRecordDataField()] = None\n",
    "    filename: Annotated[str | None, VectorStoreRecordDataField()] = None\n",
    "    content: Annotated[\n",
    "        str,\n",
    "        VectorStoreRecordDataField(\n",
    "            has_embedding=True, embedding_property_name=\"content_vector\", is_full_text_searchable=True\n",
    "        ),\n",
    "    ] = None\n",
    "    content_vector: Annotated[\n",
    "        list[float] | None,\n",
    "        VectorStoreRecordVectorField(\n",
    "            dimensions=1536,\n",
    "            local_embedding=True,\n",
    "            embedding_settings={\"embedding\": OpenAIEmbeddingPromptExecutionSettings(dimensions=1536)},\n",
    "        ),\n",
    "    ] = None\n",
    "\n",
    "\n",
    "ai_search_store = AzureAISearchStore(\n",
    "    search_endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_SEARCH_KEY\"),\n",
    ")\n",
    "collection: AzureAISearchCollection = ai_search_store.get_collection(index_name, HotelBrochures)\n",
    "\n",
    "text_search = collection.create_text_search_from_vector_text_search().create_search(\n",
    "        function_name=\"hotel_search_brochure\",\n",
    "        description=\"A search engine for hotel brochures in different cities. Use this tool to lookup for places to stay.\",\n",
    "        parameters=[\n",
    "            KernelParameterMetadata(\n",
    "                name=\"query\", description=\"The question.\", type=\"str\", is_required=True, type_object=str\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"top\",\n",
    "                description=\"Number of results to return.\",\n",
    "                type=\"int\",\n",
    "                default_value=2,\n",
    "                type_object=int,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "kernel.add_function(\n",
    "    plugin_name=\"azure_ai_search\",\n",
    "    function=text_search,\n",
    ")\n",
    "\n",
    "# üöÄ Test It!\n",
    "print(\"--- üìã Results ---\")\n",
    "result = await kernel.invoke_prompt(\n",
    "    prompt=\"Where can I stay in Las Vegas?\",\n",
    "    service_id=\"azure_openai_chat\",\n",
    "    arguments=KernelArguments(PromptExecutionSettings(function_choice_behavior=FunctionChoiceBehavior.Auto()))\n",
    ")\n",
    "print(str(result)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a084c4",
   "metadata": {},
   "source": [
    "### üß™ Try it out!\n",
    "\n",
    "You‚Äôve just added a **RAG (Retrieval-Augmented Generation)** plugin using your own hotel brochure data.\n",
    "\n",
    "üí° Try asking:\n",
    "\n",
    "- *\"Can you find me the most expensive hotel?\"*\n",
    "- *\"Any hotels for cultural appreciation?\"*\n",
    "\n",
    "üîç Notice:\n",
    "- It **doesn‚Äôt rely on general model knowledge**.\n",
    "- It works even without **exact keyword matches**, thanks to **semantic search**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
